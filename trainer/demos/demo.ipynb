{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Code Blocks for Testing\n",
    "\n",
    "**All code blocks listed in this section is required for testing. So please make sure you run them before testing, even if you don't want to train a model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    # Reset the map and set the position of rat at the given location,\n",
    "    # passed in by rat argument\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    # This function is called for updating states,\n",
    "    # act() will call this function.\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    # This function is called for perform action and change the current state\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    # This function is called to get the current map.\n",
    "    # Just indication of wall, path, and rat. No visited mark.\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Section\n",
    "\n",
    "You can run below code blocks to test the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_test = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1., 1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0., 1.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0., 1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1., 1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1., 0.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1., 0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1., 0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1., 1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to run a game episode with given model and maze map\n",
    "def test_trainer(model, maze):\n",
    "    qmaze = Qmaze(maze)\n",
    "    game_over = False\n",
    "    envstate = qmaze.observe()\n",
    "    show(qmaze)\n",
    "\n",
    "    while not game_over:\n",
    "        plt.pause(0.1)\n",
    "        valid_actions = qmaze.valid_actions()\n",
    "        if not valid_actions: break\n",
    "        prev_envstate = envstate\n",
    "\n",
    "        action = np.argmax(model.predict(prev_envstate))\n",
    "\n",
    "        # Apply action, get reward and new envstate\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        show(qmaze)\n",
    "        display.clear_output(wait=True)\n",
    "        if game_status == 'win':\n",
    "            game_over = True\n",
    "        elif game_status == 'lose':\n",
    "            game_over = True\n",
    "        else:\n",
    "            game_over = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAGR0lEQVR4nO3dMU6UjRqG4XcOlhgTMJmGhISGxg4WIHvQuIOfBQydsaQwGWojKzCRPTALkMKShkRjYkiOWEin5DvFH5PfROVwDr7yjNeVUIk+3xS3MzS8o2EYCrj5/vW7HwD474gVQogVQogVQogVQogVQty6yjcvLi4Oy8vLv+pZvvH58+d6//59y9ba2lp9+fKlZevWrVtzudW9N69bHz58qPPz89F3n+Mq/9Dy8nI9fvz4ep7qEp8+faqdnZ2WradPn9bZ2VnL1tLS0lxude/N69bu7u4P/8zHYAghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVghxpV/y3Wl1dbWeP3/+ux/jl9je3m7ZmU6nbb8ovarq8PCwHjx40LK1t7fX9tqm02ndvn27ZetnRpddPh+NRn9V1V9VVXfv3t149uxZx3PVwsJCXVxczOXWyclJy9bKykq9e/euZauqan19vRYXF1u2Tk9P217byspKLSwstGxNJpN68+bN/3Y+YxiG/arar6paXV0d5vFkQfdW5ztC9zvr/fv3W7b+xHdWP7NCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCiCudz3j79u1cnn7o3rrsCsJ1mc1mbVtf9/h1rnQ+486dOxtPnjzpeK7W0w/dW+PxuGXr/Py87ZxF996feD7j0li/+ebRqO2/6Xl+Z51MJi1bs9ms7ZxF9968ns/Y3d39Yax+ZoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQVzqfMR6PN168eNHxXK2nGOZ56/j4uGWrqvcMydraWl1cXLRsLSwstG397HzGpYephmHYr6r9qqrNzc2h6zxC5ymGed7qOjFR1XuG5OXLl3V2dtaytbS01Lb1Mz4GQwixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQohLfyP/73J0dFRbW1stW9PpdG63LjuPcp1ms1nb3sHBQcvOV9vb261733Njb92cnp623U3pvNHSvTUej1u2qnrv+Hz8+LH11s3JyUnL1s7OTg3D8N1bN5fG+k+bm5vDq1evru3BfmZvb6/tbkrnjZburclk0rJV1XvH5+DgoPXWzcOHD1u2quqHsfqZFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFULc2PMZnTY2NtrOPnSemJjNZi079HA+o6rW19fbzj50npjo3Orecz7jEvN6PuPw8LDt7EPniYnOre495zOAG0usEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEOLGxvr1pEXHV7fRaNTydXR01P7a+HVu7PmMeT0zcX5+XsfHxy1bKysrNR6PW7aqnM+4DpHnM+b1zMRsNqutra2Wrel0WpPJpGWryvmM6+J8BoQTK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4QQK4RwPuM3bDmf8f/rPp/R9bp2dnbq9evX3/2N/Lcu+8vDMOxX1X7V3+cz5vWkRefWzs5Oy9Z0Oq1Hjx61bFXN9/mMe/futWz9jI/BEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEOJK5zOqar2qem4/VN2tqn/bitnq3pvXrfVhGG5/7w8ujfV3GY1Gr4Zh2LSVsdW99ydu+RgMIcQKIW5yrPu2ora69/64rRv7MyvwrZv8zgr8g1ghhFghhFghhFghxH8A+M+57SXUf6QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the model and load the weight from the pre-trained data\n",
    "model_test = build_model(maze_test)\n",
    "model_test.load_weights('./model.h5')\n",
    "\n",
    "# Run the game\n",
    "test_trainer(model_test, maze_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Code Blocks for Training\n",
    "\n",
    "All code blocks listed here is not required for *testing*. No need to run them if you don't want to train a model for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock a behavior of playing the game\n",
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether our model can win no matter where we place the rat\n",
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning implementation\n",
    "# Remembering every episode\n",
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Section\n",
    "\n",
    "You can run this section's code block to train your model based on your defined maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x161a45dec08>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1., 1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0., 1.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0., 1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1., 1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1., 0.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1., 0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1., 0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1., 1.]\n",
    "])\n",
    "\n",
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/14999 | Loss: 0.0746 | Episodes: 134 | Win count: 1 | Win rate: 0.000 | time: 12.3 seconds\n",
      "Epoch: 001/14999 | Loss: 0.0689 | Episodes: 17 | Win count: 2 | Win rate: 0.000 | time: 14.0 seconds\n",
      "Epoch: 002/14999 | Loss: 0.1027 | Episodes: 5 | Win count: 3 | Win rate: 0.000 | time: 14.4 seconds\n",
      "Epoch: 003/14999 | Loss: 0.0209 | Episodes: 20 | Win count: 4 | Win rate: 0.000 | time: 16.3 seconds\n",
      "Epoch: 004/14999 | Loss: 0.0123 | Episodes: 4 | Win count: 5 | Win rate: 0.000 | time: 16.7 seconds\n",
      "Epoch: 005/14999 | Loss: 0.0050 | Episodes: 106 | Win count: 6 | Win rate: 0.000 | time: 26.8 seconds\n",
      "Epoch: 006/14999 | Loss: 0.0049 | Episodes: 4 | Win count: 7 | Win rate: 0.000 | time: 27.2 seconds\n",
      "Epoch: 007/14999 | Loss: 0.0052 | Episodes: 5 | Win count: 8 | Win rate: 0.000 | time: 27.7 seconds\n",
      "Epoch: 008/14999 | Loss: 0.0021 | Episodes: 75 | Win count: 9 | Win rate: 0.000 | time: 34.7 seconds\n",
      "Epoch: 009/14999 | Loss: 0.0033 | Episodes: 8 | Win count: 10 | Win rate: 0.000 | time: 35.5 seconds\n",
      "Epoch: 010/14999 | Loss: 0.0021 | Episodes: 81 | Win count: 11 | Win rate: 0.000 | time: 43.1 seconds\n",
      "Epoch: 011/14999 | Loss: 0.0036 | Episodes: 9 | Win count: 12 | Win rate: 0.000 | time: 44.0 seconds\n",
      "Epoch: 012/14999 | Loss: 0.0025 | Episodes: 7 | Win count: 13 | Win rate: 0.000 | time: 44.7 seconds\n",
      "Epoch: 013/14999 | Loss: 0.0031 | Episodes: 1 | Win count: 14 | Win rate: 0.000 | time: 44.8 seconds\n",
      "Epoch: 014/14999 | Loss: 0.0022 | Episodes: 11 | Win count: 15 | Win rate: 0.000 | time: 45.8 seconds\n",
      "Epoch: 015/14999 | Loss: 0.0016 | Episodes: 13 | Win count: 16 | Win rate: 0.000 | time: 47.0 seconds\n",
      "Epoch: 016/14999 | Loss: 0.0013 | Episodes: 9 | Win count: 17 | Win rate: 0.000 | time: 47.9 seconds\n",
      "Epoch: 017/14999 | Loss: 0.0026 | Episodes: 132 | Win count: 17 | Win rate: 0.000 | time: 60.4 seconds\n",
      "Epoch: 018/14999 | Loss: 0.0036 | Episodes: 79 | Win count: 18 | Win rate: 0.000 | time: 67.9 seconds\n",
      "Epoch: 019/14999 | Loss: 0.0030 | Episodes: 12 | Win count: 19 | Win rate: 0.000 | time: 69.1 seconds\n",
      "Epoch: 020/14999 | Loss: 0.0030 | Episodes: 2 | Win count: 20 | Win rate: 0.000 | time: 69.3 seconds\n",
      "Epoch: 021/14999 | Loss: 0.0022 | Episodes: 11 | Win count: 21 | Win rate: 0.000 | time: 70.3 seconds\n",
      "Epoch: 022/14999 | Loss: 0.0044 | Episodes: 14 | Win count: 22 | Win rate: 0.000 | time: 71.6 seconds\n",
      "Epoch: 023/14999 | Loss: 0.0052 | Episodes: 57 | Win count: 23 | Win rate: 0.000 | time: 77.0 seconds\n",
      "Epoch: 024/14999 | Loss: 0.0025 | Episodes: 6 | Win count: 24 | Win rate: 0.000 | time: 77.6 seconds\n",
      "Epoch: 025/14999 | Loss: 0.0020 | Episodes: 11 | Win count: 25 | Win rate: 0.000 | time: 78.6 seconds\n",
      "Epoch: 026/14999 | Loss: 0.0018 | Episodes: 7 | Win count: 26 | Win rate: 0.000 | time: 79.3 seconds\n",
      "Epoch: 027/14999 | Loss: 0.0010 | Episodes: 18 | Win count: 27 | Win rate: 0.000 | time: 81.0 seconds\n",
      "Epoch: 028/14999 | Loss: 0.0023 | Episodes: 22 | Win count: 28 | Win rate: 0.000 | time: 83.2 seconds\n",
      "Epoch: 029/14999 | Loss: 0.0016 | Episodes: 18 | Win count: 29 | Win rate: 0.000 | time: 84.9 seconds\n",
      "Epoch: 030/14999 | Loss: 0.0037 | Episodes: 18 | Win count: 30 | Win rate: 0.000 | time: 86.6 seconds\n",
      "Epoch: 031/14999 | Loss: 0.0005 | Episodes: 6 | Win count: 31 | Win rate: 0.000 | time: 87.2 seconds\n",
      "Epoch: 032/14999 | Loss: 0.0007 | Episodes: 5 | Win count: 32 | Win rate: 0.969 | time: 87.6 seconds\n",
      "Epoch: 033/14999 | Loss: 0.0001 | Episodes: 18 | Win count: 33 | Win rate: 0.969 | time: 89.3 seconds\n",
      "Epoch: 034/14999 | Loss: 0.0011 | Episodes: 8 | Win count: 34 | Win rate: 0.969 | time: 90.1 seconds\n",
      "Epoch: 035/14999 | Loss: 0.0007 | Episodes: 12 | Win count: 35 | Win rate: 0.969 | time: 91.2 seconds\n",
      "Epoch: 036/14999 | Loss: 0.0055 | Episodes: 68 | Win count: 36 | Win rate: 0.969 | time: 97.6 seconds\n",
      "Epoch: 037/14999 | Loss: 0.0009 | Episodes: 16 | Win count: 37 | Win rate: 0.969 | time: 99.2 seconds\n",
      "Epoch: 038/14999 | Loss: 0.0011 | Episodes: 2 | Win count: 38 | Win rate: 0.969 | time: 99.3 seconds\n",
      "Epoch: 039/14999 | Loss: 0.0016 | Episodes: 5 | Win count: 39 | Win rate: 0.969 | time: 99.8 seconds\n",
      "Epoch: 040/14999 | Loss: 0.0006 | Episodes: 18 | Win count: 40 | Win rate: 0.969 | time: 101.5 seconds\n",
      "Epoch: 041/14999 | Loss: 0.0007 | Episodes: 17 | Win count: 41 | Win rate: 0.969 | time: 103.2 seconds\n",
      "Epoch: 042/14999 | Loss: 0.0002 | Episodes: 9 | Win count: 42 | Win rate: 0.969 | time: 104.0 seconds\n",
      "Epoch: 043/14999 | Loss: 0.0006 | Episodes: 19 | Win count: 43 | Win rate: 0.969 | time: 105.8 seconds\n",
      "Epoch: 044/14999 | Loss: 0.0024 | Episodes: 9 | Win count: 44 | Win rate: 0.969 | time: 106.6 seconds\n",
      "Epoch: 045/14999 | Loss: 0.0080 | Episodes: 12 | Win count: 45 | Win rate: 0.969 | time: 107.7 seconds\n",
      "Epoch: 046/14999 | Loss: 0.0008 | Episodes: 8 | Win count: 46 | Win rate: 0.969 | time: 108.5 seconds\n",
      "Epoch: 047/14999 | Loss: 0.0009 | Episodes: 11 | Win count: 47 | Win rate: 0.969 | time: 109.5 seconds\n",
      "Epoch: 048/14999 | Loss: 0.0008 | Episodes: 18 | Win count: 48 | Win rate: 0.969 | time: 111.4 seconds\n",
      "Epoch: 049/14999 | Loss: 0.0021 | Episodes: 12 | Win count: 49 | Win rate: 1.000 | time: 112.5 seconds\n",
      "Epoch: 050/14999 | Loss: 0.0026 | Episodes: 15 | Win count: 50 | Win rate: 1.000 | time: 114.0 seconds\n",
      "Epoch: 051/14999 | Loss: 0.0004 | Episodes: 14 | Win count: 51 | Win rate: 1.000 | time: 115.4 seconds\n",
      "Epoch: 052/14999 | Loss: 0.0018 | Episodes: 14 | Win count: 52 | Win rate: 1.000 | time: 116.8 seconds\n",
      "Epoch: 053/14999 | Loss: 0.0005 | Episodes: 16 | Win count: 53 | Win rate: 1.000 | time: 118.4 seconds\n",
      "Epoch: 054/14999 | Loss: 0.0005 | Episodes: 4 | Win count: 54 | Win rate: 1.000 | time: 118.9 seconds\n",
      "Epoch: 055/14999 | Loss: 0.0004 | Episodes: 6 | Win count: 55 | Win rate: 1.000 | time: 119.5 seconds\n",
      "Epoch: 056/14999 | Loss: 0.0007 | Episodes: 10 | Win count: 56 | Win rate: 1.000 | time: 120.6 seconds\n",
      "Epoch: 057/14999 | Loss: 0.0008 | Episodes: 3 | Win count: 57 | Win rate: 1.000 | time: 120.9 seconds\n",
      "Epoch: 058/14999 | Loss: 0.0031 | Episodes: 65 | Win count: 58 | Win rate: 1.000 | time: 127.1 seconds\n",
      "Epoch: 059/14999 | Loss: 0.0013 | Episodes: 51 | Win count: 59 | Win rate: 1.000 | time: 132.2 seconds\n",
      "Epoch: 060/14999 | Loss: 0.0009 | Episodes: 4 | Win count: 60 | Win rate: 1.000 | time: 132.6 seconds\n",
      "Epoch: 061/14999 | Loss: 0.0007 | Episodes: 14 | Win count: 61 | Win rate: 1.000 | time: 134.1 seconds\n",
      "Epoch: 062/14999 | Loss: 0.0008 | Episodes: 3 | Win count: 62 | Win rate: 1.000 | time: 134.4 seconds\n",
      "Epoch: 063/14999 | Loss: 0.0002 | Episodes: 8 | Win count: 63 | Win rate: 1.000 | time: 135.3 seconds\n",
      "Epoch: 064/14999 | Loss: 0.0018 | Episodes: 7 | Win count: 64 | Win rate: 1.000 | time: 136.0 seconds\n",
      "Epoch: 065/14999 | Loss: 0.0002 | Episodes: 17 | Win count: 65 | Win rate: 1.000 | time: 137.7 seconds\n",
      "Epoch: 066/14999 | Loss: 0.0005 | Episodes: 11 | Win count: 66 | Win rate: 1.000 | time: 138.9 seconds\n",
      "Epoch: 067/14999 | Loss: 0.0003 | Episodes: 8 | Win count: 67 | Win rate: 1.000 | time: 139.7 seconds\n",
      "Epoch: 068/14999 | Loss: 0.0019 | Episodes: 2 | Win count: 68 | Win rate: 1.000 | time: 140.0 seconds\n",
      "Epoch: 069/14999 | Loss: 0.0014 | Episodes: 9 | Win count: 69 | Win rate: 1.000 | time: 141.0 seconds\n",
      "Epoch: 070/14999 | Loss: 0.0012 | Episodes: 5 | Win count: 70 | Win rate: 1.000 | time: 141.5 seconds\n",
      "Epoch: 071/14999 | Loss: 0.0011 | Episodes: 30 | Win count: 71 | Win rate: 1.000 | time: 144.5 seconds\n",
      "Epoch: 072/14999 | Loss: 0.0005 | Episodes: 14 | Win count: 72 | Win rate: 1.000 | time: 146.0 seconds\n",
      "Epoch: 073/14999 | Loss: 0.0052 | Episodes: 23 | Win count: 73 | Win rate: 1.000 | time: 148.3 seconds\n",
      "Epoch: 074/14999 | Loss: 0.0007 | Episodes: 5 | Win count: 74 | Win rate: 1.000 | time: 148.9 seconds\n",
      "Epoch: 075/14999 | Loss: 0.0002 | Episodes: 17 | Win count: 75 | Win rate: 1.000 | time: 150.6 seconds\n",
      "Epoch: 076/14999 | Loss: 0.0004 | Episodes: 12 | Win count: 76 | Win rate: 1.000 | time: 151.9 seconds\n",
      "Epoch: 077/14999 | Loss: 0.0006 | Episodes: 21 | Win count: 77 | Win rate: 1.000 | time: 154.1 seconds\n",
      "Epoch: 078/14999 | Loss: 0.0007 | Episodes: 2 | Win count: 78 | Win rate: 1.000 | time: 154.6 seconds\n",
      "Epoch: 079/14999 | Loss: 0.0004 | Episodes: 5 | Win count: 79 | Win rate: 1.000 | time: 155.3 seconds\n",
      "Epoch: 080/14999 | Loss: 0.0006 | Episodes: 5 | Win count: 80 | Win rate: 1.000 | time: 155.8 seconds\n",
      "Epoch: 081/14999 | Loss: 0.0006 | Episodes: 19 | Win count: 81 | Win rate: 1.000 | time: 157.7 seconds\n",
      "Epoch: 082/14999 | Loss: 0.0013 | Episodes: 13 | Win count: 82 | Win rate: 1.000 | time: 159.0 seconds\n",
      "Epoch: 083/14999 | Loss: 0.0005 | Episodes: 10 | Win count: 83 | Win rate: 1.000 | time: 160.3 seconds\n",
      "Epoch: 084/14999 | Loss: 0.0004 | Episodes: 10 | Win count: 84 | Win rate: 1.000 | time: 161.4 seconds\n",
      "Epoch: 085/14999 | Loss: 0.0013 | Episodes: 13 | Win count: 85 | Win rate: 1.000 | time: 162.8 seconds\n",
      "Epoch: 086/14999 | Loss: 0.0011 | Episodes: 7 | Win count: 86 | Win rate: 1.000 | time: 163.6 seconds\n",
      "Epoch: 087/14999 | Loss: 0.0011 | Episodes: 20 | Win count: 87 | Win rate: 1.000 | time: 165.6 seconds\n",
      "Epoch: 088/14999 | Loss: 0.0010 | Episodes: 4 | Win count: 88 | Win rate: 1.000 | time: 166.2 seconds\n",
      "Reached 100% win rate at epoch: 88\n",
      "files: model.h5, model.json\n",
      "n_epoch: 88, max_mem: 512, data: 32, time: 166.5 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "166.509707"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
